{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import necessary dependencies\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path, device\n",
    "\n",
    "PATH = './data'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts for classifier path \n",
    "# texts are organized into folders of classes\n",
    "def get_texts_class(path, classes, train_pct):\n",
    "    texts, labels = [], []\n",
    "    for c in classes: \n",
    "        for fname in (path/c).glob('*.*'):\n",
    "            texts.append(fname.open('r', encoding='utf-8').read())\n",
    "            labels.append(classes.index(c))\n",
    "    texts, labels = np.array(texts), np.array(labels)\n",
    "    cols = ['labels', 'text']\n",
    "    df = pd.DataFrame({'text': texts, 'labels': labels}, columns=cols)\n",
    "    rand_idx = np.random.permutation(len(df))\n",
    "    df = df.loc[rand_idx]\n",
    "    train_df = df.iloc[:int((train_pct * len(df)))]\n",
    "    val_df = df.iloc[int((train_pct * len(df))):]   \n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def manual_rand(df, train_pct):\n",
    "    rand_idx = np.random.permutation(len(df))\n",
    "    df = df.loc[rand_idx]\n",
    "    train_df = df.iloc[:int((train_pct * len(df)))]\n",
    "    val_df = df.iloc[int((train_pct * len(df))):]   \n",
    "    return train_df, val_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataframe, isolate cols\n",
    "data_df = pd.read_csv('train.csv', encoding='latin-1')\n",
    "data_df = data_df[['Sentiment', 'SentimentText']]\n",
    "\n",
    "# create toy df\n",
    "toy_df = data_df[:1000]\n",
    "train_df, valid_df = manual_rand(toy_df, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>@georgediaz #Magic ..thinking less than 50 % ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>0</td>\n",
       "      <td>I am actually crying now because of sore thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1</td>\n",
       "      <td>@DWO34 Great to follow @BUTTERFLYWHEEL @Debra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>0</td>\n",
       "      <td>Dont Wanna Leave NY Today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>I hate it when any athlete appears to tear ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment                                      SentimentText\n",
       "200          0   @georgediaz #Magic ..thinking less than 50 % ...\n",
       "519          0    I am actually crying now because of sore thr...\n",
       "755          1   @DWO34 Great to follow @BUTTERFLYWHEEL @Debra...\n",
       "907          0                         Dont Wanna Leave NY Today.\n",
       "30           0     I hate it when any athlete appears to tear ..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(f'{PATH}/train.csv', index=False, header=False)\n",
    "test_df.to_csv(f'{PATH}/valid.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fields\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "fields = [\n",
    "    ('label', LABEL),\n",
    "    ('text', TEXT)\n",
    "]\n",
    "\n",
    "# Create datasets\n",
    "train_data, valid_data = data.TabularDataset.splits(path=PATH,\n",
    "                                                    train='train.csv',\n",
    "                                                    test ='valid.csv',\n",
    "                                                    format='csv',\n",
    "                                                    fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '0', 'text': ['-', '@littlecharva', '-', 'you', \"'ll\", 'be', 'pleased', 'to', 'know', 'I', 'did', 'not', 'have', 'a', 'nice', 'night', \"'s\", 'sleep', ',', 'even', 'in', 'a', 'comfy', 'bed', ';)', 'even', 'a', 'sleeping', 'bag', 'would', 'be', 'same']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(valid_data.examples[71]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [03:18, 4.34MB/s]                               \n",
      "100%|█████████▉| 398797/400000 [00:18<00:00, 22803.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data length: 700\n",
      "training vocab length: 3154\n",
      "label vocab length: 3154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 398797/400000 [00:30<00:00, 22803.84it/s]"
     ]
    }
   ],
   "source": [
    "# Build the vocab using GloVE vectors, 100 dim\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=25000, vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train_data)\n",
    "print(f'training data length: {len(train_data)}')\n",
    "print(f'training vocab length: {len(TEXT.vocab)}')\n",
    "print(f'label vocab length: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset iterators\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model \n",
    "\n",
    "class biLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                          bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        #concatenate final forward, backward layers\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set structural parameters\n",
    "\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 5\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = biLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "           N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pretrained GloVE embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "# replace the weights of the model embedding layer with pretrained embeddings\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# set optimizer to Adam\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set GPU\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy function\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds)).float()\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # keeps dropout on\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label.float())\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # turns dropout off\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label.float())\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.666 | Train Acc: 65.84% | Val. Loss: 0.646 | Val. Acc: 67.64% |\n",
      "| Epoch: 02 | Train Loss: 0.638 | Train Acc: 65.71% | Val. Loss: 0.608 | Val. Acc: 67.64% |\n",
      "| Epoch: 03 | Train Loss: 0.614 | Train Acc: 66.33% | Val. Loss: 0.583 | Val. Acc: 70.00% |\n",
      "| Epoch: 04 | Train Loss: 0.581 | Train Acc: 71.20% | Val. Loss: 0.523 | Val. Acc: 73.30% |\n",
      "| Epoch: 05 | Train Loss: 0.558 | Train Acc: 72.98% | Val. Loss: 0.543 | Val. Acc: 71.31% |\n",
      "| Epoch: 06 | Train Loss: 0.522 | Train Acc: 76.71% | Val. Loss: 0.503 | Val. Acc: 78.41% |\n",
      "| Epoch: 07 | Train Loss: 0.456 | Train Acc: 80.49% | Val. Loss: 0.462 | Val. Acc: 78.86% |\n",
      "| Epoch: 08 | Train Loss: 0.468 | Train Acc: 79.47% | Val. Loss: 0.421 | Val. Acc: 81.19% |\n",
      "| Epoch: 09 | Train Loss: 0.430 | Train Acc: 81.88% | Val. Loss: 0.526 | Val. Acc: 76.11% |\n",
      "| Epoch: 10 | Train Loss: 0.430 | Train Acc: 81.43% | Val. Loss: 0.401 | Val. Acc: 81.68% |\n",
      "| Epoch: 11 | Train Loss: 0.391 | Train Acc: 83.99% | Val. Loss: 0.370 | Val. Acc: 82.87% |\n",
      "| Epoch: 12 | Train Loss: 0.355 | Train Acc: 85.26% | Val. Loss: 0.339 | Val. Acc: 85.23% |\n",
      "| Epoch: 13 | Train Loss: 0.334 | Train Acc: 88.27% | Val. Loss: 0.292 | Val. Acc: 88.49% |\n",
      "| Epoch: 14 | Train Loss: 0.266 | Train Acc: 90.01% | Val. Loss: 0.409 | Val. Acc: 83.52% |\n",
      "| Epoch: 15 | Train Loss: 0.219 | Train Acc: 90.73% | Val. Loss: 0.312 | Val. Acc: 88.32% |\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the result for a single input\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_class(input):\n",
    "    # tokenize the input\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(input)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    pred = torch.round(prediction)\n",
    "    if pred == 0: print('negative')\n",
    "    else: print('positive')\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04189132899045944"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class(\"suck a dick bill gates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6655281186103821"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class(\"I'm just really happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
